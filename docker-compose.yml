version: '3.8'

services:
  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - "2181:2181"
    networks:
      - kafka_network

  kafka:
    image: wurstmeister/kafka
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_LOG_RETENTION_MS: "86400000" 
      KAFKA_LOG_SEGMENT_BYTES: "134217728"  # Set to 128 MB per segment
      KAFKA_LOG_CLEANER_THREADS: "4"  # Use multiple cleaner threads
      KAFKA_MAX_REQUEST_SIZE: "52428800"  # Increase max message size to 50 MB
      KAFKA_MESSAGE_MAX_BYTES: "52428800"
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    networks:
      kafka_network:
        aliases:
          - kafka
    ulimits:
      nofile:
        soft: 100000
        hard: 100000
    logging:
      driver: "none"  # Suppress logging


  flink-jobmanager:
    build:  # Use Dockerfile in the current directory
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env  # Load environment variables from the .env file
    environment:
      JOB_MANAGER_RPC_ADDRESS: flink-jobmanager
      SMTP_RECV: ${SMTP_RECV}
      SMTP_PASS: ${SMTP_PASS}
      SMTP_USER: ${SMTP_USER}
    ports:
      - "8081:8081"
    command: jobmanager
    networks:
      - kafka_network
    volumes:
      - ./flink-sql-connector-kafka-3.3.0-1.20.jar:/opt/flink/lib/flink-sql-connector-kafka-3.3.0-1.20.jar
      - /tmp:/tmp
    restart: always


  flink-taskmanager:
    build: .
    environment:
      JOB_MANAGER_RPC_ADDRESS: flink-jobmanager
      TASK_MANAGER_NUMBER_OF_TASK_SLOTS: 14  # Set the number of task slots set based on the resources of machine
      FLINK_PROPERTIES: "taskmanager.numberOfTaskSlots=14"
    depends_on:
      - flink-jobmanager
    command: taskmanager
    networks:
      - kafka_network
    volumes:
      - ./flink-sql-connector-kafka-3.3.0-1.20.jar:/opt/flink/lib/flink-sql-connector-kafka-3.3.0-1.20.jar
      - /tmp:/tmp
    deploy:
      replicas: 1
    restart: always 


  stream-emulator:
    build: .
    volumes:
      - ./data:/opt/flink/jobs/data
      - ./:/usr/src/app
    command: ["./wait-for-it.sh", "kafka:9092", "--", "./retry_stream_emulator.sh"]
    depends_on:
      - kafka
    networks:
      - kafka_network
 

  influxdb:
    image: influxdb:2.7
    ports:
      - "8086:8086"
    env_file:
      - .env
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: admin
      DOCKER_INFLUXDB_INIT_PASSWORD: admin123
      DOCKER_INFLUXDB_INIT_ORG: ticker_org
      DOCKER_INFLUXDB_INIT_BUCKET: ticker_bucket
      DOCKER_INFLUXDB_INIT_RETENTION: 1h  # Retain data for 1 hour
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: ${INFLUXDB_TOKEN}
    networks:
      - kafka_network

      
  grafana:
    image: grafana/grafana-oss:10.0.3
    ports:
      - "3000:3000"
    depends_on:
      - influxdb
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin123
    networks:
      - kafka_network
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana:/etc/grafana/provisioning/dashboards


  kafka_to_influx:
    build:
      context: docker/.
      dockerfile: Dockerfile
    depends_on:
      - kafka
      - influxdb
    env_file:
      - .env
    networks:
      - kafka_network

networks:
  kafka_network:
    driver: bridge
  
volumes:
  grafana-data:
  influxdb-data:
