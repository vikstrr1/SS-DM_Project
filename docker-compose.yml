version: '3.8'

services:
  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - "2181:2181"
    networks:
      - kafka_network

  kafka:
    image: wurstmeister/kafka
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_LOG_RETENTION_MS: "86400000" 
      KAFKA_LOG_SEGMENT_BYTES: "134217728"  # Set to 128 MB per segment
      KAFKA_LOG_CLEANER_THREADS: "4"  # Use multiple cleaner threads
      KAFKA_MAX_REQUEST_SIZE: "52428800"  # Increase max message size to 50 MB
      KAFKA_MESSAGE_MAX_BYTES: "52428800"
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    networks:
      kafka_network:
        aliases:
          - kafka
    ulimits:
      nofile:
        soft: 100000
        hard: 100000


  flink-jobmanager:
    build:  # Use Dockerfile in the current directory
      context: .
      dockerfile: Dockerfile
    env_file:
      - .env  # Load environment variables from the .env file
    environment:
      JOB_MANAGER_RPC_ADDRESS: flink-jobmanager
      SMTP_RECV: ${SMTP_RECV}
      SMTP_PASS: ${SMTP_PASS}
      SMTP_USER: ${SMTP_USER}
    ports:
      - "8081:8081"
    command: ["jobmanager", "sh", "-c", "cd .. && ./bin/flink run -py /opt/flink/jobs/flink_processor.py"]
    networks:
      - kafka_network
      - elastic_network
    volumes:
      - ./flink-sql-connector-kafka-3.3.0-1.20.jar:/opt/flink/lib/flink-sql-connector-kafka-3.3.0-1.20.jar
      - ./data/trading_data:/opt/flink/jobs/data/trading_data
      - ./flink-sql-connector-elasticsearch7-3.0.1-1.17.jar:/opt/flink/lib/flink-connector-elasticsearch.jar
      - /tmp:/tmp
    restart: always

  flink-taskmanager:
    build: .
    environment:
      JOB_MANAGER_RPC_ADDRESS: flink-jobmanager
      TASK_MANAGER_NUMBER_OF_TASK_SLOTS: 4  # Set the number of task slots set based on the resources of machine
      FLINK_PROPERTIES: "taskmanager.numberOfTaskSlots=4"
    depends_on:
      - flink-jobmanager
    command: taskmanager
    networks:
      - kafka_network
    volumes:
      - ./flink-sql-connector-kafka-3.3.0-1.20.jar:/opt/flink/lib/flink-sql-connector-kafka-3.3.0-1.20.jar
      - ./data/trading_data:/opt/flink/jobs/data/trading_data
      - ./flink-sql-connector-elasticsearch7-3.0.1-1.17.jar:/opt/flink/lib/flink-connector-elasticsearch.jar
      - /tmp:/tmp
    deploy:
      replicas: 1
    restart: always  

  stream-emulator:
    build: .
    volumes:
      - ./data:/opt/flink/jobs/data
      - ./:/usr/src/app
    command: ["./wait-for-it.sh", "kafka:9092", "--", "./retry_stream_emulator.sh"]
    depends_on:
      - kafka
    networks:
      - kafka_network

  #dashboard:
  #  build: .
  #  command: ["./wait-for-it.sh", "kafka:9092", "--", "python", "visualization_server.py"]
  #  ports:
  #    - "8050:8050"
  #  depends_on:
  #    - kafka
  #  networks:
  #    - kafka_network

  #elasticsearch:
  #  image: docker.elastic.co/elasticsearch/elasticsearch:8.1.0
  #  environment:
  #    - discovery.type=single-node
  #    - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
  #    - ELASTIC_PASSWORD=password123
  #  ports:
  #    - "9200:9200"
  #  networks:
  #    - elastic_network

  #kibana:
  #  image: docker.elastic.co/kibana/kibana:8.1.0
  #  ports:
  #    - "5601:5601"
  #  networks:
  #    - elastic_network
  #  depends_on:
  #    - elasticsearch
  influxdb:
    image: influxdb:2.7
    ports:
      - "8086:8086"
    env_file:
      - .env
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: admin
      DOCKER_INFLUXDB_INIT_PASSWORD: admin123
      DOCKER_INFLUXDB_INIT_ORG: ticker_org
      DOCKER_INFLUXDB_INIT_BUCKET: ticker_bucket
      DOCKER_INFLUXDB_INIT_RETENTION: 1h  # Retain data for 1 hour
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: ${INFLUXDB_TOKEN}
    networks:
      - kafka_network
      
  grafana:
    image: grafana/grafana-oss:10.0.3
    ports:
      - "3000:3000"
    depends_on:
      - influxdb
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_DATASOURCE_NAME=influxdb
      - GF_DATASOURCE_TYPE=influxdb
      - GF_DATASOURCE_URL=http://influxdb:8086
      - GF_DATASOURCE_ACCESS=proxy
      - GF_DATASOURCE_INFLUXDB_VERSION=2
      - GF_DATASOURCE_INFLUXDB_TOKEN=${INFLUXDB_TOKEN}  # Ensure token is read correctly from .env file
      - GF_DATASOURCE_INFLUXDB_ORG=${DOCKER_INFLUXDB_INIT_ORG}
      - GF_DATASOURCE_INFLUXDB_BUCKET=${DOCKER_INFLUXDB_INIT_BUCKET}
    networks:
      - kafka_network
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana:/etc/grafana/provisioning/dashboards

  kafka_to_influx:
    build:
      context: docker/.
      dockerfile: Dockerfile
    depends_on:
      - kafka
      - influxdb
    env_file:
      - .env
    networks:
      - kafka_network
networks:
  kafka_network:
    driver: bridge
volumes:
  grafana-data:
  influxdb-data:
